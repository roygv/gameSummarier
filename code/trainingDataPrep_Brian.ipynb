{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.09 s, sys: 259 ms, total: 1.35 s\n",
      "Wall time: 1.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "from scipy import fftpack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline\n",
    "\n",
    "from os import sep\n",
    "from os.path import isfile, join, abspath, expanduser\n",
    "import platform\n",
    "\n",
    "from moviepy.editor import AudioFileClip, VideoFileClip\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "from ggplot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356  events loaded\n"
     ]
    }
   ],
   "source": [
    "#home = expanduser(\"~\")\n",
    "dirpath = join(abspath(sep),'Users/brianschneider/Desktop/capstone/gameSummarizer')\n",
    "file_basename = 'trainingData'\n",
    "eventFileName = join(dirpath, file_basename+'.csv')\n",
    "event_data = pd.read_csv(eventFileName, sep=',',header=0)\n",
    "print(len(event_data),' events loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute offset of each clip in the new clips collection and add to DataFrame\n",
    "offset=np.cumsum(event_data[\"timeToCapture\"])\n",
    "event_data[\"offset\"]=offset-event_data[\"timeToCapture\"]\n",
    "# Prepare a list of labels\n",
    "labels=[row[\"eventName\"] \n",
    "        for index,row in event_data.iterrows() \n",
    "        for i in np.arange(row[\"offset\"],row[\"offset\"]+row[\"timeToCapture\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a sound array from training data and perform FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "F = []\n",
    "samprate = 44100\n",
    "for index, row in event_data.iterrows():\n",
    "    #print row['secondOffset'], row['timeToCapture']\n",
    "    videoFileName = join(dirpath, row['fileName'])\n",
    "    if isfile(videoFileName):\n",
    "        audioClip = AudioFileClip(videoFileName).subclip(row['secondOffset'],row['secondOffset']+row['timeToCapture'])\n",
    "        wavClip = audioClip.to_soundarray(fps=samprate)\n",
    "        wavdata = wavClip.reshape(-1, samprate,  2)\n",
    "        dims = wavdata.shape\n",
    "        for sec in np.arange(dims[0]):\n",
    "            ch1 = scipy.fftpack.fft(wavdata[sec,:,0])[:samprate//2] # Left channel\n",
    "            ch2 = scipy.fftpack.fft(wavdata[sec,:,1])[:samprate//2] # Right channel\n",
    "            ch = np.vstack([ch1,ch2])\n",
    "            F.append(ch)\n",
    "        del audioClip\n",
    "    else:\n",
    "        print('Could not find file ', videoFileName)\n",
    "\n",
    "F = np.absolute(F)/samprate\n",
    "F_db = 20*np.log10(2*F)  # Dimensions are (seconds, channels, samples)\n",
    "f = scipy.fftpack.fftfreq(samprate, 1.0/samprate)[:samprate//2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a video clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def getClip(path,offset,length):\n",
    "#     videoClip = VideoFileClip(path).subclip(offset,offset+length)\n",
    "#     print('Playing {0}, offset={1}, length={2}'.format(path,offset,length))\n",
    "#     return videoClip\n",
    "\n",
    "# def getEvent(eventNumber):\n",
    "#     offset=int(event_data.iloc[eventNumber-1][\"secondOffset\"])\n",
    "#     length=int(event_data.iloc[eventNumber-1][\"timeToCapture\"])\n",
    "#     path=join(dirpath, event_data.iloc[eventNumber-1][\"fileName\"])\n",
    "#     videoClip = VideoFileClip(path).subclip(offset,offset+length)\n",
    "#     print('Playing {0}, offset={1}, length={2}'.format(path,offset,length))\n",
    "#     return videoClip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play an event number from event file (first event is #1 which is in line #2 in the file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clip = getEvent(2)  \n",
    "# clip.ipython_display(width=480, autoplay=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play a portion of a clip using offset and length in seconds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot frequency vs. amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotFrequencySpectrum(second):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(f,np.transpose(F_db[second-1,:,:]), alpha=0.5)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Amplitude(in dB)')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(f,np.transpose(F[second-1,:,:]), alpha=0.4)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(wavdata[second-1,:,], alpha=0.4)\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFrequencySpectrum(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimentionality reduction: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a handy function for vizualizing principle components as a heatmap\n",
    "# this allows you to see what dimensions in the 'original space' are\n",
    "# active\n",
    "\n",
    "def visualize_pca_comps_heatmap(plot, comps):\n",
    "    heatmap = plot.pcolor(comps, cmap=plt.cm.Blues)\n",
    "    \n",
    "    x_lab = [i for i in range(comps.shape[1])]\n",
    "    y_lab = [i for i in (range(comps.shape[0]))]\n",
    "    \n",
    "    plot.set_xticks(np.arange(comps.shape[1])+0.5, minor=False)\n",
    "    plot.set_yticks(np.arange(comps.shape[0])+0.5, minor=False)\n",
    "    \n",
    "    # want a more natural, table-like display\n",
    "    plot.invert_yaxis()\n",
    "    \n",
    "    plot.set_xticklabels(x_lab, minor=False)\n",
    "    plot.set_yticklabels(y_lab, minor=False)\n",
    "    \n",
    "    plt.title('Heatmap of PCA components Rows: components, Cols: Original dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (F_db.shape)\n",
    "F2 = F.reshape(F_db.shape[0],-1)\n",
    "print (F2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_components = 170\n",
    "pca_model = PCA(n_components)\n",
    "pca_model.fit(F2)\n",
    "joblib.dump(pca_model, 'model_pca.pkl', protocol=2) \n",
    "print('Explained variance ratio: \\n', pca_model.explained_variance_ratio_[-20:])\n",
    "print('Explained variance ratio: \\n', pca_model.explained_variance_ratio_.shape)\n",
    "print('Cumulative explained variance: \\n', np.cumsum(pca_model.explained_variance_ratio_)[-20:])\n",
    "print('PCA components: \\n', pca_model.components_.shape)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "#p2 = plt.subplot(1, 2, 1)\n",
    "#visualize_pca_comps_heatmap(p2, model.components_)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.cumsum(pca_model.explained_variance_ratio_))\n",
    "plt.xlabel('Num components')\n",
    "plt.ylabel('explained variance (in %)')\n",
    "plt.title('PCA components vs. explained variance ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply PCA Transformation to the input to create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(F2.shape)\n",
    "t= pca_model.transform(F2)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(t[:,0], columns=[\"pca-one\"])\n",
    "df_pca[\"pca-two\"] = t[:,1]\n",
    "df_pca[\"labels\"] = labels\n",
    "\n",
    "\n",
    "chart = ggplot( df_pca, aes(x='pca-one', y='pca-two', color='labels') ) \\\n",
    "        + geom_point(size=75,alpha=0.4) \\\n",
    "        + ggtitle(\"First and Second Principal Components colored by event type\")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try tSNE\n",
    "Use the reduced dimensions as were produced by PCA (to improve performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.manifold import TSNE\n",
    "# tsne = TSNE(n_components=2, verbose=1, perplexity=160, n_iter=1200, learning_rate=25)\n",
    "# tsne_results = tsne.fit_transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_tsne = pd.DataFrame(tsne_results[:,0], columns=[\"x-tsne\"])\n",
    "# df_tsne[\"y-tsne\"] = tsne_results[:,1]\n",
    "# df_tsne[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chart = ggplot( df_tsne, aes(x='x-tsne', y='y-tsne', color='labels') ) \\\n",
    "#         + geom_point(size=70,alpha=0.5) \\\n",
    "#         + ggtitle(\"tSNE dimensions colored by event type\")\n",
    "# chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Try truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# svd = TruncatedSVD(n_components=n_components, n_iter=17, random_state=42)\n",
    "# svd_results = svd.fit_transform(F2)\n",
    "# print(svd.explained_variance_ratio_) \n",
    "# print(svd.explained_variance_ratio_.sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_svd = pd.DataFrame(svd_results[:,0], columns=[\"svd-one\"])\n",
    "# df_svd[\"svd-two\"] = svd_results[:,1]\n",
    "# df_svd[\"labels\"] = labels\n",
    "\n",
    "\n",
    "# chart = ggplot( df_svd, aes(x='svd-one', y='svd-two', color='labels') ) \\\n",
    "#         + geom_point(size=75,alpha=0.6) \\\n",
    "#         + ggtitle(\"First and Second SVD components colored by event type\")\n",
    "# chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "index=np.array(np.arange(len(labels)))\n",
    "data=pca_model.transform(F2)\n",
    "data_train, data_test, labels_train, labels_test, index_train, index_test = train_test_split(data, labels, index, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=4)\n",
    "model.fit(data_train, labels_train)\n",
    "joblib.dump(model, 'model_knn.pkl', protocol=2) \n",
    "\n",
    "test_predicted_labels = model.predict(data_test)\n",
    "\n",
    "wrong_prediction = (test_predicted_labels != labels_test)\n",
    "\n",
    "print('number of incorrect predictions: {0} out of {1}'.format(np.sum(wrong_prediction), len(labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print('accuracy: {0:.3g}%'.format(100*float(np.sum((test_predicted_labels == labels_test)))/len(labels_test)))\n",
    "print(\"F1 score: {0:.3g}%\".format(100*f1_score(labels_test, test_predicted_labels, average=\"weighted\")))\n",
    "print(\"Precision: {0:.3g}%\".format(100*precision_score(labels_test, test_predicted_labels, average=\"weighted\")))\n",
    "print(\"Recall: : {0:.3g}%\".format(100*recall_score(labels_test, test_predicted_labels, average=\"weighted\")))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(labels_test, test_predicted_labels)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"Goal\",\"MissedShot\",\"NonEvent\"],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event=[index\n",
    "        for index,row in event_data.iterrows() \n",
    "        for i in np.arange(row[\"offset\"],row[\"offset\"]+row[\"timeToCapture\"])]\n",
    "second=[i-row[\"offset\"]+1\n",
    "        for index,row in event_data.iterrows() \n",
    "        for i in np.arange(row[\"offset\"],row[\"offset\"]+row[\"timeToCapture\"])]\n",
    "confused=index_test[(np.array(labels_test) == 'goal') & (np.array(test_predicted_labels) == 'nonevent')]\n",
    "event=np.array(event)\n",
    "second=np.array(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixups=event_data.ix[event[confused]]\n",
    "mixups[\"second\"]=second[confused]\n",
    "print(\"'secondOffset' is offset in the original clip\")\n",
    "print(\"'offset' is offset in the concatenated clip\")\n",
    "mixups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gra = GradientBoostingClassifier()\n",
    "gra.fit(data_train, labels_train)\n",
    "joblib.dump(model, 'model_gra.pkl', protocol=2) \n",
    "print('Accuracy (GBC):', gra.score(data_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_labels_gra = gra.predict(data_test)\n",
    "wrong_prediction_gra = (test_predicted_labels_gra != labels_test)\n",
    "print('number of incorrect predictions: {0} out of {1}'.format(np.sum(wrong_prediction), len(labels_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {0:.3g}%'.format(100*float(np.sum((test_predicted_labels_gra == labels_test)))/len(labels_test)))\n",
    "print(\"F1 score: {0:.3g}%\".format(100*f1_score(labels_test, test_predicted_labels_gra, average=\"weighted\")))\n",
    "print(\"Precision: {0:.3g}%\".format(100*precision_score(labels_test, test_predicted_labels_gra, average=\"weighted\")))\n",
    "print(\"Recall: : {0:.3g}%\".format(100*recall_score(labels_test, test_predicted_labels_gra, average=\"weighted\")))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "model_svm = svm.SVC(C=100000)\n",
    "model_svm.fit(data_train, labels_train)\n",
    "joblib.dump(model_svm, 'model_svm.pkl', protocol=2) \n",
    "test_predicted_labels = model_svm.predict(data_test)\n",
    "wrong_prediction = (test_predicted_labels != labels_test)\n",
    "print('number of incorrect predictions: {0} out of {1}'.format(np.sum(wrong_prediction), len(labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print('accuracy: {0:.3g}%'.format(100*float(np.sum((test_predicted_labels == labels_test)))/len(labels_test)))\n",
    "print(\"F1 score: {0:.3g}%\".format(100*f1_score(labels_test, test_predicted_labels, average=\"weighted\")))\n",
    "print(\"Precision: {0:.3g}%\".format(100*precision_score(labels_test, test_predicted_labels, average=\"weighted\")))\n",
    "print(\"Recall: : {0:.3g}%\".format(100*recall_score(labels_test, test_predicted_labels, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=200, max_features=100, bootstrap=True, oob_score=True, min_samples_leaf=2)\n",
    "rfc.fit(data_train, labels_train)\n",
    "\n",
    "print('Accuracy (a random forest):', rfc.score(data_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_labels_rfc = rfc.predict(data_test)\n",
    "\n",
    "wrong_prediction_rfc = (test_predicted_labels_rfc != labels_test)\n",
    "\n",
    "print('number of incorrect predictions:', np.sum(wrong_prediction_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {0:.3g}%'.format(100*float(np.sum((test_predicted_labels_rfc == labels_test)))/len(labels_test)))\n",
    "print(\"F1 score: {0:.3g}%\".format(100*f1_score(labels_test, test_predicted_labels_rfc, average=\"weighted\")))\n",
    "print(\"Precision: {0:.3g}%\".format(100*precision_score(labels_test, test_predicted_labels_rfc, average=\"weighted\")))\n",
    "print(\"Recall: : {0:.3g}%\".format(100*recall_score(labels_test, test_predicted_labels_rfc, average=\"weighted\")))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnf_matrix = confusion_matrix(labels_test, test_predicted_labels_rfc)\n",
    "# np.set_printoptions(precision=2)\n",
    "\n",
    "# # Plot non-normalized confusion matrix\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=[\"Whistle\",\"CardGiven\",\"Goal\",\"MissedShot\",\"NonEvent\"],\n",
    "#                       title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=4)\n",
    "clf2 = RandomForestClassifier(n_estimators=200, max_features=100, bootstrap=True, oob_score=True, min_samples_leaf=2)\n",
    "clf3 = GradientBoostingClassifier()\n",
    "clf4 = svm.SVC(C=100000)\n",
    "\n",
    "eclf3 = VotingClassifier(estimators=[\n",
    "       ('knn', clf1), ('rf', clf2), ('gbc', clf3), ('svm', clf3)],\n",
    "       voting='soft', weights=[2,1,3,1])\n",
    "eclf3 = eclf3.fit(data_train, labels_train)\n",
    "print('Accuracy (ensemble):', eclf3.score(data_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(eclf3, 'model_ensemble.pkl', protocol=2) \n",
    "test_predicted_labels_eclf3 = eclf3.predict(data_test)\n",
    "wrong_prediction_eclf3 = (test_predicted_labels_eclf3 != labels_test)\n",
    "print('number of incorrect predictions: {0} out of {1}'.format(np.sum(wrong_prediction_eclf3), len(labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {0:.3g}%'.format(100*float(np.sum((test_predicted_labels_eclf3 == labels_test)))/len(labels_test)))\n",
    "print(\"F1 score: {0:.3g}%\".format(100*f1_score(labels_test, test_predicted_labels_eclf3, average=\"weighted\")))\n",
    "print(\"Precision: {0:.3g}%\".format(100*precision_score(labels_test, test_predicted_labels_eclf3, average=\"weighted\")))\n",
    "print(\"Recall: : {0:.3g}%\".format(100*recall_score(labels_test, test_predicted_labels_eclf3, average=\"weighted\")))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event=[index\n",
    "        for index,row in event_data.iterrows() \n",
    "        for i in np.arange(row[\"offset\"],row[\"offset\"]+row[\"timeToCapture\"])]\n",
    "second=[i-row[\"offset\"]+1\n",
    "        for index,row in event_data.iterrows() \n",
    "        for i in np.arange(row[\"offset\"],row[\"offset\"]+row[\"timeToCapture\"])]\n",
    "confused=index_test[(np.array(labels_test) == 'goal') & (np.array(test_predicted_labels_eclf3) == 'nonevent')]\n",
    "event=np.array(event)\n",
    "second=np.array(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixups=event_data.ix[event[confused]]\n",
    "mixups[\"second\"]=second[confused]\n",
    "print(\"'secondOffset' is offset in the original clip\")\n",
    "print(\"'offset' is offset in the concatenated clip\")\n",
    "mixups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(labels_test, test_predicted_labels_eclf3)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"Goal\",\"MissedShot\",\"NonEvent\"],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import GaussianNB \n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from mlxtend.classifier import EnsembleVoteClassifier\n",
    "\n",
    "# clf1 = LogisticRegression(random_state=1)\n",
    "# clf2 = RandomForestClassifier(random_state=1)\n",
    "# clf3 = GradientBoostingClassifier()\n",
    "# eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n",
    "\n",
    "# params = {'logisticregression__C': [1.0, 100.0],\n",
    "#           'randomforestclassifier__n_estimators': [20, 200, 500, 1000],}\n",
    "\n",
    "# grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "# grid.fit(data_train, labels_train)\n",
    "\n",
    "# cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "\n",
    "# for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n",
    "#     print(\"%0.3f +/- %0.2f %r\"\n",
    "#           % (grid.cv_results_[cv_keys[0]][r],\n",
    "#              grid.cv_results_[cv_keys[1]][r] / 2.0,\n",
    "#              grid.cv_results_[cv_keys[2]][r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
