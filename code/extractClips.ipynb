{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "from scipy import fftpack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline\n",
    "\n",
    "from os import sep\n",
    "from os.path import isfile, join, abspath\n",
    "from moviepy.editor import AudioFileClip, VideoFileClip, concatenate_videoclips, concatenate_audioclips\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from ggplot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = join(abspath(sep),'temp')\n",
    "file_basename = 'trainingData'\n",
    "eventFileName = join('.', file_basename+'.csv')\n",
    "event_data = pd.read_csv(eventFileName, sep=',',header=0)\n",
    "print len(event_data),' events loaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute offset of each clip in the new clips collection and add to DataFrame\n",
    "offset=np.cumsum(event_data[\"timeToCapture\"])\n",
    "event_data[\"offset\"]=offset-event_data[\"timeToCapture\"]\n",
    "# Prepare a list of labels\n",
    "labels=[row[\"eventName\"] \n",
    "        for index,row in event_data.iterrows() \n",
    "        for i in np.arange(row[\"offset\"],row[\"offset\"]+row[\"timeToCapture\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate video clips using ffmpeg (fast but audio gets out of sync)\n",
    "This approach needs more work. In the mean time use the slower videoPy option below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for index, row in event_data.iterrows():\n",
    "#    #print row['secondOffset'], row['timeToCapture']\n",
    "#    if row['fileName'] == file_basename+'.wav':\n",
    "#        outFileName=os.path.join(dirpath, file_basename+str(index)+'.mp4')\n",
    "#        print 'Writing clip #',index,' from sec ',row['secondOffset'],' length ',row['timeToCapture']\n",
    "#        ffmpeg_extract_subclip(videoFileName, row['secondOffset'], row['secondOffset']+row['timeToCapture'], targetname=outFileName)\n",
    "#    else:\n",
    "#        print 'Skipping ', row['fileName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://trac.ffmpeg.org/wiki/Concatenate\n",
    "Getting error: \"IOPub data rate exceeded\"?\n",
    "\n",
    "Add the following as a parameter to 'Jupyter notebook': --NotebookApp.iopub_data_rate_limit=50000000\n",
    "or upgrade Jupyter notebook to 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate video clips using videoPy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clips=[]\n",
    "for index, row in event_data.iterrows():\n",
    "    #print row['secondOffset'], row['timeToCapture']\n",
    "    videoFileName = join(dirpath, row['fileName'])\n",
    "    if isfile(videoFileName):\n",
    "        videoClip = VideoFileClip(videoFileName).subclip(row['secondOffset'],row['secondOffset']+row['timeToCapture'])\n",
    "        clips.append(videoClip)\n",
    "    else:\n",
    "        print 'Could not find file ', videoFileName\n",
    "    \n",
    "final_clip = concatenate_videoclips(clips)\n",
    "final_clip.write_videofile(join(dirpath, \"my_concatenation.mp4\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for clip in clips:\n",
    "    del clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a video clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility: Play a video clip from file starting at <offset> (in seconds) for <length> (in seconds)\n",
    "def getClip(path=join(dirpath, \"my_concatenation.mp4\"),offset=0,length=30):\n",
    "    videoClip = VideoFileClip(path).subclip(offset,offset+length)\n",
    "    return videoClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getEvent(eventNumber):\n",
    "    o=int(event_data.iloc[eventNumber-1][\"offset\"])\n",
    "    l=int(event_data.iloc[eventNumber-1][\"timeToCapture\"])\n",
    "    return getClip(offset=o, length=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play an event number from event file (first event is #1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = getEvent(44)  \n",
    "clip.ipython_display(width=640, autoplay=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play a portion of a clip using offset and length in seconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip = getClip(offset=0,length=15) \n",
    "clip.ipython_display(width=640, autoplay=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate audio clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clips = []\n",
    "for index, row in event_data.iterrows():\n",
    "    #print row['secondOffset'], row['timeToCapture']\n",
    "    videoFileName = join(dirpath, row['fileName'])\n",
    "    if isfile(videoFileName):\n",
    "        audioClip = AudioFileClip(videoFileName).subclip(row['secondOffset'],row['secondOffset']+row['timeToCapture'])\n",
    "        clips.append(audioClip)\n",
    "    else:\n",
    "        print 'Could not find file ', videoFileName\n",
    "\n",
    "final_clip = concatenate_audioclips(clips)\n",
    "final_clip.write_audiofile(join(dirpath, \"my_concatenation.mp3\"))\n",
    "#final_clip.write_videofile(\"/temp/my_concatenation.mp3\")\n",
    "#final_clip.ipython_display(\"/temp/my_concatenation.mp3\",width=640, autoplay=True)\n",
    "for clip in clips:\n",
    "    del clip.reader\n",
    "    del clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play first 30 seconds of audio clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clip.set_end(30).ipython_display(join(dirpath, \"my_concatenation.mp3\"),width=640, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert audio to WAV for ease of audio processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  import subprocess\n",
    "  audioFile = join(dirpath, \"my_concatenation.mp3\")\n",
    "  subprocess.call(['ffmpeg', '-y', '-i', audioFile,\n",
    "                   join(dirpath, \"my_concatenation.wav\")])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform FFT on audio in WAV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " audioFile = join(dirpath, \"my_concatenation.wav\")  \n",
    "samprate, wavdata = scipy.io.wavfile.read(audioFile)\n",
    "clipDurationInSec = wavdata.shape[0]/samprate\n",
    "# wavdata is (clipLengthInSec * samprate) X (2 channels: [0] is left, [1] is right )\n",
    "# Sample rate is 44,100\n",
    "print samprate\n",
    "print wavdata.shape[0]/samprate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "F = []\n",
    "# Perform fft, take only positive freq. (22,050 samples) \n",
    "#  then calulate the absolute value to include both real and imaginary parts\n",
    "wavdata2 = wavdata.reshape(-1, samprate,  2)\n",
    "# wavdata2 = wavdata.reshape(wavdata.shape[0]/samprate, samprate,  2L)\n",
    "dims = wavdata2.shape\n",
    "for sec in np.arange(dims[0]):\n",
    "    ch1 = scipy.fftpack.fft(wavdata2[sec,:,0])[:samprate//2] # Left channel\n",
    "    ch2 = scipy.fftpack.fft(wavdata2[sec,:,1])[:samprate//2] # Right channel\n",
    "    ch = np.vstack([ch1,ch2])\n",
    "    F.append(ch)\n",
    "F = np.absolute(F)/samprate\n",
    "F_db = 20*np.log10(2*F)  # Dimensions are (seconds, channels, samples)\n",
    "f = scipy.fftpack.fftfreq(samprate, 1.0/samprate)[:samprate//2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot frequency vs. amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotFrequencySpectrum(second):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(f,np.transpose(F_db[second-1,:,:]), alpha=0.5)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Amplitude(in dB)')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(f,np.transpose(F[second-1,:,:]), alpha=0.4)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(wavdata2[second-1,:,], alpha=0.4)\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFrequencySpectrum(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimentionality reduction: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a handy function for vizualizing principle components as a heatmap\n",
    "# this allows you to see what dimensions in the 'original space' are\n",
    "# active\n",
    "\n",
    "def visualize_pca_comps_heatmap(plot, comps):\n",
    "    heatmap = plot.pcolor(comps, cmap=plt.cm.Blues)\n",
    "    \n",
    "    x_lab = [i for i in range(comps.shape[1])]\n",
    "    y_lab = [i for i in (range(comps.shape[0]))]\n",
    "    \n",
    "    plot.set_xticks(np.arange(comps.shape[1])+0.5, minor=False)\n",
    "    plot.set_yticks(np.arange(comps.shape[0])+0.5, minor=False)\n",
    "    \n",
    "    # want a more natural, table-like display\n",
    "    plot.invert_yaxis()\n",
    "    \n",
    "    plot.set_xticklabels(x_lab, minor=False)\n",
    "    plot.set_yticklabels(y_lab, minor=False)\n",
    "    \n",
    "    plt.title('Heatmap of PCA components Rows: components, Cols: Original dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (F.shape)\n",
    "F2 = F.reshape(F.shape[0],-1)\n",
    "print (F2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_components = 150\n",
    "pca_model = PCA(n_components)\n",
    "pca_model.fit(F2)\n",
    "\n",
    "print('Explained variance ratio: \\n', pca_model.explained_variance_ratio_[-20:])\n",
    "print('Explained variance ratio: \\n', pca_model.explained_variance_ratio_.shape)\n",
    "print('Cumulative explained variance: \\n', np.cumsum(pca_model.explained_variance_ratio_)[-20:])\n",
    "print('PCA components: \\n', pca_model.components_.shape)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "#p2 = plt.subplot(1, 2, 1)\n",
    "#visualize_pca_comps_heatmap(p2, model.components_)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.cumsum(pca_model.explained_variance_ratio_))\n",
    "plt.xlabel('Num components')\n",
    "plt.ylabel('explained variance (in %)')\n",
    "plt.title('PCA components vs. explained variance ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply PCA Transformation to the input to create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print F2.shape\n",
    "t= pca_model.transform(F2)\n",
    "print t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(t[:,0], columns=[\"pca-one\"])\n",
    "df_pca[\"pca-two\"] = t[:,1]\n",
    "df_pca[\"labels\"] = labels\n",
    "\n",
    "\n",
    "chart = ggplot( df_pca, aes(x='pca-one', y='pca-two', color='labels') ) \\\n",
    "        + geom_point(size=75,alpha=0.4) \\\n",
    "        + ggtitle(\"First and Second Principal Components colored by event type\")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try tSNE\n",
    "Use the reduced dimensions as were produced by PCA (to improve performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=900, learning_rate=20)\n",
    "tsne_results = tsne.fit_transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(tsne_results[:,0], columns=[\"x-tsne\"])\n",
    "df_tsne[\"y-tsne\"] = tsne_results[:,1]\n",
    "df_tsne[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = ggplot( df_tsne, aes(x='x-tsne', y='y-tsne', color='labels') ) \\\n",
    "        + geom_point(size=70,alpha=0.5) \\\n",
    "        + ggtitle(\"tSNE dimensions colored by event type\")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Try truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=n_components, n_iter=17, random_state=42)\n",
    "svd_results = svd.fit_transform(F2)\n",
    "print(svd.explained_variance_ratio_) \n",
    "print(svd.explained_variance_ratio_.sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svd = pd.DataFrame(svd_results[:,0], columns=[\"svd-one\"])\n",
    "df_svd[\"svd-two\"] = svd_results[:,1]\n",
    "df_svd[\"labels\"] = labels\n",
    "\n",
    "\n",
    "chart = ggplot( df_svd, aes(x='svd-one', y='svd-two', color='labels') ) \\\n",
    "        + geom_point(size=75,alpha=0.6) \\\n",
    "        + ggtitle(\"First and Second SVD components colored by event type\")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "index=np.array(np.arange(len(labels)))\n",
    "data=pca_model.transform(F2)\n",
    "data_train, data_test, labels_train, labels_test, index_train, index_test = train_test_split(data, labels, index, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(data_train, labels_train)\n",
    "test_predicted_labels = model.predict(data_test)\n",
    "\n",
    "wrong_prediction = (test_predicted_labels != labels_test)\n",
    "\n",
    "print 'number of incorrect predictions:', np.sum(wrong_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print('accuracy: {0:.3g}%'.format(100*float(np.sum((test_predicted_labels == labels_test)))/len(labels_test)))\n",
    "print(\"F1 score: {0:.3g}%\".format(100*f1_score(labels_test, test_predicted_labels, average=\"macro\")))\n",
    "print(\"Precision: {0:.3g}%\".format(100*precision_score(labels_test, test_predicted_labels, average=\"macro\")))\n",
    "print(\"Recall: : {0:.3g}%\".format(100*recall_score(labels_test, test_predicted_labels, average=\"macro\")))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(labels_test, test_predicted_labels)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"Whistle\",\"CardGiven\",\"Goal\",\"MissedShot\",\"NonEvent\"],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l=['#'+str(index+1)+': '+row[\"fileName\"]+','+str(row[\"secondOffset\"])+','+str(i-row[\"offset\"]+1)+','+row[\"eventName\"]\n",
    "        for index,row in event_data.iterrows() \n",
    "        for i in np.arange(row[\"offset\"],row[\"offset\"]+row[\"timeToCapture\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1=index_test[(np.array(labels_test) == 'goal') & (np.array(test_predicted_labels) == 'nonevent')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l=np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[l1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
