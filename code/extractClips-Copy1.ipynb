{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "from scipy import fftpack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline\n",
    "\n",
    "from os import sep\n",
    "from os.path import isfile, join, abspath\n",
    "from moviepy.editor import AudioFileClip, VideoFileClip, concatenate_videoclips, concatenate_audioclips\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from ggplot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirpath = join(abspath(sep),'Users/brianschneider/desktop/capstone/gamesummarizer')\n",
    "file_basename = 'trainingData'\n",
    "eventFileName = join('.', file_basename+'.csv')\n",
    "event_data = pd.read_csv(eventFileName, sep=',',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute offset of each clip in the new clips collection and add to DataFrame\n",
    "offset=np.cumsum(event_data[\"timeToCapture\"])\n",
    "event_data[\"offset\"]=offset-event_data[\"timeToCapture\"]\n",
    "# Prepare a list of labels\n",
    "labels=[row[\"eventName\"] \n",
    "        for index,row in event_data.iterrows() \n",
    "        for i in np.arange(row[\"offset\"],row[\"offset\"]+row[\"timeToCapture\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate video clips using ffmpeg (fast but audio gets out of sync)\n",
    "This approach needs more work. In the mean time use the slower videoPy option below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for index, row in event_data.iterrows():\n",
    "#    #print row['secondOffset'], row['timeToCapture']\n",
    "#    if row['fileName'] == file_basename+'.wav':\n",
    "#        outFileName=os.path.join(dirpath, file_basename+str(index)+'.mp4')\n",
    "#        print 'Writing clip #',index,' from sec ',row['secondOffset'],' length ',row['timeToCapture']\n",
    "#        ffmpeg_extract_subclip(videoFileName, row['secondOffset'], row['secondOffset']+row['timeToCapture'], targetname=outFileName)\n",
    "#    else:\n",
    "#        print 'Skipping ', row['fileName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://trac.ffmpeg.org/wiki/Concatenate\n",
    "Getting error: \"IOPub data rate exceeded\"?\n",
    "\n",
    "Add the following as a parameter to 'Jupyter notebook': --NotebookApp.iopub_data_rate_limit=50000000\n",
    "or upgrade Jupyter notebook to 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate video clips using videoPy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 24] Too many open files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-afb5bfc89083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clips=[]\\nfor index, row in event_data.iterrows():\\n    #print row[\\'secondOffset\\'], row[\\'timeToCapture\\']\\n    videoFileName = join(dirpath, row[\\'fileName\\'])\\n    if isfile(videoFileName):\\n        videoClip = VideoFileClip(videoFileName).subclip(row[\\'secondOffset\\'],row[\\'secondOffset\\']+row[\\'timeToCapture\\'])\\n        clips.append(videoClip)\\n    else:\\n        print(\\'Could not find file \\', videoFileName)\\n\\nfinal_clip = concatenate_videoclips(clips)\\nfinal_clip.write_videofile(join(dirpath, \"my_concatenation.mp4\"))'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/brianschneider/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/brianschneider/anaconda/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brianschneider/anaconda/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/brianschneider/anaconda/lib/python3.5/site-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m    109\u001b[0m                                        \u001b[0mbuffersize\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0maudio_buffersize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                                        \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_fps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                                        nbytes = audio_nbytes)\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brianschneider/anaconda/lib/python3.5/site-packages/moviepy/audio/io/AudioFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, buffersize, nbytes, fps)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         reader = FFMPEG_AudioReader(filename,fps=fps,nbytes=nbytes,\n\u001b[0;32m---> 63\u001b[0;31m                                          buffersize=buffersize)\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brianschneider/anaconda/lib/python3.5/site-packages/moviepy/audio/io/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, buffersize, print_infos, fps, nbytes, nchannels)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macodec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pcm_s%dle'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_parse_infos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'duration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'video_duration'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brianschneider/anaconda/lib/python3.5/site-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mpopen_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"creationflags\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0x08000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brianschneider/anaconda/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    948\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brianschneider/anaconda/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1452\u001b[0m             \u001b[0;31m# Data format: \"exception name:hex errno:description\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m             \u001b[0;31m# Pickle is not used; it is complex and involves memory allocation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m             \u001b[0;31m# errpipe_write must not be in the standard io 0, 1, or 2 fd range.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m             \u001b[0mlow_fds_to_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 24] Too many open files"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clips=[]\n",
    "for index, row in event_data.iterrows():\n",
    "    #print row['secondOffset'], row['timeToCapture']\n",
    "    videoFileName = join(dirpath, row['fileName'])\n",
    "    if isfile(videoFileName):\n",
    "        videoClip = VideoFileClip(videoFileName).subclip(row['secondOffset'],row['secondOffset']+row['timeToCapture'])\n",
    "        clips.append(videoClip)\n",
    "    else:\n",
    "        print('Could not find file ', videoFileName)\n",
    "\n",
    "final_clip = concatenate_videoclips(clips)\n",
    "final_clip.write_videofile(join(dirpath, \"my_concatenation.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a video clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility: Play a video clip from file starting at <offset> (in seconds) for <length> (in seconds)\n",
    "def getClip(path=join(dirpath, \"my_concatenation.mp4\"),offset=0,length=30):\n",
    "    videoClip = VideoFileClip(path).subclip(offset,offset+length)\n",
    "    return videoClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getEvent(eventNumber):\n",
    "    o=int(event_data.iloc[eventNumber-1][\"offset\"])\n",
    "    l=int(event_data.iloc[eventNumber-1][\"timeToCapture\"])\n",
    "    return getClip(offset=o, length=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play an event number from event file (first event is #1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip = getEvent(20)  \n",
    "clip.ipython_display(width=640, autoplay=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play a portion of a clip using offset and length in seconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip = getClip(offset=0,length=15) \n",
    "clip.ipython_display(width=640, autoplay=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate audio clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clips = []\n",
    "for index, row in event_data.iterrows():\n",
    "    #print row['secondOffset'], row['timeToCapture']\n",
    "    videoFileName = join(dirpath, row['fileName'])\n",
    "    if isfile(videoFileName):\n",
    "        audioClip = AudioFileClip(videoFileName).subclip(row['secondOffset'],row['secondOffset']+row['timeToCapture'])\n",
    "        clips.append(audioClip)\n",
    "    else:\n",
    "        print 'Could not find file ', videoFileName\n",
    "\n",
    "final_clip = concatenate_audioclips(clips)\n",
    "final_clip.write_audiofile(join(dirpath, \"my_concatenation.mp3\"))\n",
    "#final_clip.write_videofile(\"/temp/my_concatenation.mp3\")\n",
    "#final_clip.ipython_display(\"/temp/my_concatenation.mp3\",width=640, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play first 30 seconds of audio clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_clip.set_end(30).ipython_display(join(dirpath, \"my_concatenation.mp3\"),width=640, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert audio to WAV for ease of audio processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  import subprocess\n",
    "  audioFile = join(dirpath, \"my_concatenation.mp3\")\n",
    "  subprocess.call(['ffmpeg', '-y', '-i', audioFile,\n",
    "                   join(dirpath, \"my_concatenation.wav\")])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform FFT on audio in WAV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " audioFile = join(dirpath, \"my_concatenation.wav\")  \n",
    "samprate, wavdata = scipy.io.wavfile.read(audioFile)\n",
    "clipDurationInSec = wavdata.shape[0]/samprate\n",
    "# wavdata is (clipLengthInSec * samprate) X (2 channels: [0] is left, [1] is right )\n",
    "# Sample rate is 44,100\n",
    "print samprate\n",
    "print wavdata.shape[0]/samprate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "F = []\n",
    "# Perform fft, take only positive freq. (22,050 samples) \n",
    "#  then calulate the absolute value to include both real and imaginary parts\n",
    "wavdata2 = wavdata.reshape(-1, samprate,  2L)\n",
    "# wavdata2 = wavdata.reshape(wavdata.shape[0]/samprate, samprate,  2L)\n",
    "dims = wavdata2.shape\n",
    "for sec in np.arange(dims[0]):\n",
    "    ch1 = scipy.fftpack.fft(wavdata2[sec,:,0])[:samprate//2] # Left channel\n",
    "    ch2 = scipy.fftpack.fft(wavdata2[sec,:,1])[:samprate//2] # Right channel\n",
    "    ch = np.vstack([ch1,ch2])\n",
    "    F.append(ch)\n",
    "F = np.absolute(F)/samprate\n",
    "F_db = 20*np.log10(2*F)  # Dimensions are (seconds, channels, samples)\n",
    "f = scipy.fftpack.fftfreq(samprate, 1.0/samprate)[:samprate//2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot frequency vs. amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotFrequencySpectrum(second):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(f,np.transpose(F_db[second-1,:,:]), alpha=0.5)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Amplitude(in dB)')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(f,np.transpose(F[second-1,:,:]), alpha=0.4)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotFrequencySpectrum(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimentionality reduction: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a handy function for vizualizing principle components as a heatmap\n",
    "# this allows you to see what dimensions in the 'original space' are\n",
    "# active\n",
    "\n",
    "def visualize_pca_comps_heatmap(plot, comps):\n",
    "    heatmap = plot.pcolor(comps, cmap=plt.cm.Blues)\n",
    "    \n",
    "    x_lab = [i for i in range(comps.shape[1])]\n",
    "    y_lab = [i for i in (range(comps.shape[0]))]\n",
    "    \n",
    "    plot.set_xticks(np.arange(comps.shape[1])+0.5, minor=False)\n",
    "    plot.set_yticks(np.arange(comps.shape[0])+0.5, minor=False)\n",
    "    \n",
    "    # want a more natural, table-like display\n",
    "    plot.invert_yaxis()\n",
    "    \n",
    "    plot.set_xticklabels(x_lab, minor=False)\n",
    "    plot.set_yticklabels(y_lab, minor=False)\n",
    "    \n",
    "    plt.title('Heatmap of PCA components Rows: components, Cols: Original dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (F.shape)\n",
    "F2 = F.reshape(F.shape[0],-1)\n",
    "print (F2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_components = 20\n",
    "model = PCA(n_components)\n",
    "model.fit(F2)\n",
    "\n",
    "print 'Explained variance ratio: \\n', model.explained_variance_ratio_[-20:]\n",
    "print 'Explained variance ratio: \\n', model.explained_variance_ratio_.shape\n",
    "print 'Cumulative explained variance: \\n', np.cumsum(model.explained_variance_ratio_)[-20:]\n",
    "print 'PCA components: \\n', model.components_.shape\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "#p2 = plt.subplot(1, 2, 1)\n",
    "#visualize_pca_comps_heatmap(p2, model.components_)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.cumsum(model.explained_variance_ratio_))\n",
    "plt.xlabel('Num components')\n",
    "plt.ylabel('explained variance (in %)')\n",
    "plt.title('PCA components vs. explained variance ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply PCA Transformation to the input to create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print F2.shape\n",
    "t= model.transform(F2)\n",
    "print t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=900, learning_rate=20)\n",
    "tsne_results = tsne.fit_transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(tsne_results[:,0], columns=[\"x-tsne\"])\n",
    "df_tsne[\"y-tsne\"] = tsne_results[:,1]\n",
    "df_tsne[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chart = ggplot( df_tsne, aes(x='x-tsne', y='y-tsne', color='labels') ) \\\n",
    "        + geom_point(size=70,alpha=0.5) \\\n",
    "        + ggtitle(\"tSNE dimensions colored by event type\")\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(t[:,0], columns=[\"pca-one\"])\n",
    "df_pca[\"pca-two\"] = t[:,1]\n",
    "df_pca[\"labels\"] = labels\n",
    "\n",
    "\n",
    "chart = ggplot( df_pca, aes(x='pca-one', y='pca-two', color='labels') ) \\\n",
    "        + geom_point(size=75,alpha=0.6) \\\n",
    "        + ggtitle(\"First and Second Principal Components colored by event type\")\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=n_components, n_iter=7, random_state=42)\n",
    "svd_results = svd.fit_transform(F2)\n",
    "print(svd.explained_variance_ratio_) \n",
    "print(svd.explained_variance_ratio_.sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_svd = pd.DataFrame(svd_results[:,0], columns=[\"svd-one\"])\n",
    "df_svd[\"svd-two\"] = svd_results[:,1]\n",
    "df_svd[\"labels\"] = labels\n",
    "\n",
    "\n",
    "chart = ggplot( df_svd, aes(x='svd-one', y='svd-two', color='labels') ) \\\n",
    "        + geom_point(size=75,alpha=0.6) \\\n",
    "        + ggtitle(\"First and Second SVD components colored by event type\")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(F2, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(data_train, labels_train)\n",
    "test_predicted_labels = model.predict(data_test)\n",
    "\n",
    "wrong_prediction = (test_predicted_labels != labels_test)\n",
    "\n",
    "print 'number of incorrect predictions:', np.sum(wrong_prediction)\n",
    "print 'accuracy:', np.sum(wrong_prediction)/len(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print 'accuracy: {0:.3g}%'.format(100*float(np.sum((test_predicted_labels == labels_test)))/len(labels_test))\n",
    "print(\"F1 score: {0:.3g}%\".format(100*f1_score(labels_test, test_predicted_labels, average=\"macro\")))\n",
    "print(\"Precision: {0:.3g}%\".format(100*precision_score(labels_test, test_predicted_labels, average=\"macro\")))\n",
    "print(\"Recall: : {0:.3g}%\".format(100*recall_score(labels_test, test_predicted_labels, average=\"macro\")))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(labels_test, test_predicted_labels)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"Whistle\",\"CardGiven\",\"Goal\",\"MissedShot\",\"NonEvent\"],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
